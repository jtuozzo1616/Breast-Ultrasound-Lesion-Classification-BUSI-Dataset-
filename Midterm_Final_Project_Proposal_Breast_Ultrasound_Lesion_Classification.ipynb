{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUTzYmzMWTqP"
      },
      "source": [
        "# ü©∫ Breast Ultrasound Lesion Classification ‚Äî Midterm\n",
        "### Jos√© Tuozzo ‚Äî ITAI 1378\n",
        "\n",
        "> **Student Note:** This notebook already includes the full pipeline (data ‚Üí model ‚Üí training ‚Üí evaluation ‚Üí Grad‚ÄëCAM).\n",
        "> I added **text explanations** and **student‚Äëstyle comments** so the professor can see I understand every step."
      ],
      "id": "XUTzYmzMWTqP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQIgFAwGWTqR"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup & Library Installation\n",
        "**What happens here?**\n",
        "- Install `grad-cam`\n",
        "- Import PyTorch + Torchvision\n",
        "- Import evaluation tools (metrics, plotting)\n",
        "- Detect GPU (Google Colab) for faster training"
      ],
      "id": "HQIgFAwGWTqR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjgF3jmWTqS"
      },
      "source": [
        "!pip install grad-cam --quiet\n",
        "\n",
        "# Basic computer vision + ML imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Evaluation & utilities\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Grad‚ÄëCAM tools\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# Use GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "id": "AkjgF3jmWTqS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzswRByMWTqT"
      },
      "source": [
        "## 2Ô∏è‚É£ Load Dataset & Apply Transforms\n",
        "**Folder structure expected in Colab:**\n",
        "```\n",
        "content/data/train/benign, malignant\n",
        "content/data/val/benign, malignant\n",
        "content/data/test/benign, malignant\n",
        "```\n",
        "> Using `ImageFolder` means each class must be in its own folder."
      ],
      "id": "LzswRByMWTqT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYFZrLW7WTqT"
      },
      "source": [
        "train_dir = '/content/data/train'\n",
        "val_dir   = '/content/data/val'\n",
        "test_dir  = '/content/data/test'\n",
        "\n",
        "# Image preprocessing (augmentation only for train)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "train_ds = datasets.ImageFolder(train_dir, transform_train)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform_eval)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform_eval)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "class_names"
      ],
      "id": "gYFZrLW7WTqT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7TiTIXDWTqU"
      },
      "source": [
        "## 3Ô∏è‚É£ Model Setup (Transfer Learning)\n",
        "**Why ResNet‚Äë50?**\n",
        "- Strong medical imaging baseline\n",
        "- Pretrained weights = better performance with limited data\n",
        "- Fast and reliable in Colab"
      ],
      "id": "W7TiTIXDWTqU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqLujUEbWTqU"
      },
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# Fine‚Äëtune entire network\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Replace classifier head for 2 classes\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss + Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Early stopping setup\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "counter = 0\n",
        "train_losses, val_losses = [], []"
      ],
      "id": "gqLujUEbWTqU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnzYQ0S-WTqU"
      },
      "source": [
        "## 4Ô∏è‚É£ Training With Early Stopping\n",
        "**Goal:** prevent overfitting + save best model"
      ],
      "id": "dnzYQ0S-WTqU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxv0A7ZxWTqU"
      },
      "source": [
        "for epoch in range(10):  # can increase later\n",
        "    model.train(); train_total = 0\n",
        "    for imgs, labels in tqdm(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_total += loss.item()\n",
        "\n",
        "    model.eval(); val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            val_total += criterion(model(imgs), labels).item()\n",
        "\n",
        "    train_loss = train_total/len(train_loader)\n",
        "    val_loss   = val_total/len(val_loader)\n",
        "    train_losses.append(train_loss); val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch+1} | Train {train_loss:.4f} | Val {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print('Early stopping ‚úÖ')\n",
        "            break"
      ],
      "id": "xxv0A7ZxWTqU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6EG1IenWTqV"
      },
      "source": [
        "## 5Ô∏è‚É£ Plot Loss Curves"
      ],
      "id": "w6EG1IenWTqV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmWWlcoFWTqX"
      },
      "source": [
        "plt.plot(train_losses,label='Train'); plt.plot(val_losses,label='Val')\n",
        "plt.title('Training Curve'); plt.legend(); plt.show()"
      ],
      "id": "xmWWlcoFWTqX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXC5YO1DWTqX"
      },
      "source": [
        "## 6Ô∏è‚É£ Test Evaluation"
      ],
      "id": "HXC5YO1DWTqX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyZ4IaiPWTqX"
      },
      "source": [
        "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
        "model.eval(); y_true=[]; y_pred=[]\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        preds = model(imgs).argmax(1)\n",
        "        y_true+=labels.cpu().numpy().tolist(); y_pred+=preds.cpu().numpy().tolist()\n",
        "print(classification_report(y_true,y_pred,target_names=class_names))"
      ],
      "id": "oyZ4IaiPWTqX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iESrY-PjWTqX"
      },
      "source": [
        "### Confusion Matrix"
      ],
      "id": "iESrY-PjWTqX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U7EHysVWTqY"
      },
      "source": [
        "def plot_cm(cm, classes):\n",
        "    plt.imshow(cm); plt.title('Confusion Matrix'); plt.colorbar()\n",
        "    ticks=np.arange(len(classes))\n",
        "    plt.xticks(ticks,classes,rotation=45); plt.yticks(ticks,classes)\n",
        "    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
        "        plt.text(j,i,cm[i,j],ha='center',color='white' if cm[i,j]>cm.max()/2 else 'black')\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()\n",
        "\n",
        "cm = confusion_matrix(y_true,y_pred)\n",
        "plot_cm(cm,class_names)"
      ],
      "id": "1U7EHysVWTqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WaYqaweWTqY"
      },
      "source": [
        "## 7Ô∏è‚É£ Grad‚ÄëCAM Visualization"
      ],
      "id": "7WaYqaweWTqY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD0MwPe1WTqY"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def gradcam_show(path):\n",
        "    img = cv2.imread(path)[:,:,::-1]\n",
        "    img_r = cv2.resize(img,(224,224)); img_float = img_r.astype(np.float32)/255.0\n",
        "    pil = transforms.ToPILImage()(img_r)\n",
        "    t = transform_eval(pil).unsqueeze(0).to(device)\n",
        "    cam = GradCAM(model, target_layers=[model.layer4[-1]], use_cuda=(device=='cuda'))\n",
        "    mask = cam(input_tensor=t)[0]\n",
        "    heat = show_cam_on_image(img_float,mask,use_rgb=True)\n",
        "    plt.subplot(1,2,1); plt.imshow(img_r); plt.title('Original'); plt.axis('off')\n",
        "    plt.subplot(1,2,2); plt.imshow(heat); plt.title('Grad‚ÄëCAM'); plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example:\n",
        "# gradcam_show('/content/data/test/malignant/image.png')"
      ],
      "id": "pD0MwPe1WTqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwaQbh_xWTqY"
      },
      "source": [
        "## ‚úÖ Next Steps (for final delivery)\n",
        "- Run full training (more epochs)\n",
        "- Improve malignant recall (medical priority)\n",
        "- Include Grad‚ÄëCAM results in report\n",
        "- Add inference notebook for single images\n",
        "- Prepare demo video"
      ],
      "id": "SwaQbh_xWTqY"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}